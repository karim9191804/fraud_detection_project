"""
Mod√®le Hybride GNN+LLM - Version L√©g√®re
Fusion optimis√©e pour training rapide
"""

import torch
import torch.nn as nn


class LightHybridModel(nn.Module):
    """
    Mod√®le Hybride L√©ger
    - GNN: 64 hidden channels
    - LLM: DistilBERT (66M params)
    - Fusion: Projection simple
    
    Total: ~170M params (vs 2.8B+ pour version compl√®te)
    Avec LoRA: seulement ~1.1M params entra√Ænables
    """
    
    def __init__(self, gnn_model, llm_model):
        super().__init__()
        
        self.gnn = gnn_model
        self.llm = llm_model
        
        # Dimensions
        self.gnn_hidden = 64  # GNN output
        self.llm_hidden = 768  # DistilBERT hidden size
        
        # Projecteur GNN ‚Üí LLM
        self.projector = nn.Sequential(
            nn.Linear(self.gnn_hidden, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, self.llm_hidden)
        )
        
        # Fusion finale
        self.fusion_layer = nn.Sequential(
            nn.Linear(self.llm_hidden * 2, 256),  # GNN + LLM
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 2)  # Fraud/Not Fraud
        )
    
    def forward(self, graph_data, text_data=None):
        """
        Forward pass hybride
        
        Args:
            graph_data: (x, edge_index, batch) pour GNN
            text_data: (input_ids, attention_mask) pour LLM (optionnel)
        
        Returns:
            logits: Pr√©diction finale
            embeddings: Dict avec GNN + LLM embeddings
        """
        
        x, edge_index, batch = graph_data
        
        # 1. GNN Forward
        gnn_logits, gnn_embeddings = self.gnn(x, edge_index, batch)
        
        # 2. Projeter GNN embeddings vers espace LLM
        gnn_projected = self.projector(gnn_embeddings)
        
        # 3. Si texte fourni, utiliser LLM
        if text_data is not None:
            input_ids, attention_mask = text_data
            llm_logits, llm_embeddings = self.llm(input_ids, attention_mask)
            
            # Fusion GNN + LLM
            fused = torch.cat([gnn_projected, llm_embeddings], dim=-1)
            final_logits = self.fusion_layer(fused)
            
            embeddings = {
                'gnn': gnn_embeddings,
                'llm': llm_embeddings,
                'fused': fused
            }
        else:
            # Mode GNN seulement (plus rapide)
            final_logits = gnn_logits
            embeddings = {
                'gnn': gnn_embeddings,
                'llm': None,
                'fused': gnn_projected
            }
        
        return final_logits, embeddings
    
    def predict_with_explanation(self, graph_data, transaction_info):
        """
        Pr√©diction + Explication
        
        Utilis√© en MODE JOUR pour inf√©rence rapide
        """
        
        with torch.no_grad():
            # Pr√©diction GNN rapide
            logits, embeddings = self.forward(graph_data, text_data=None)
            
            # Probabilit√©s
            probs = torch.softmax(logits, dim=-1)
            pred_class = torch.argmax(probs, dim=-1)
            confidence = probs.max(dim=-1).values
            
            # G√©n√©rer explication simple
            explanation = self.llm.generate_explanation(
                embeddings['gnn'],
                transaction_info
            )
        
        return {
            'prediction': pred_class,
            'confidence': confidence,
            'probabilities': probs,
            'explanation': explanation,
            'embeddings': embeddings
        }
    
    def freeze_gnn(self):
        """Geler GNN pour fine-tuning LLM seulement"""
        for param in self.gnn.parameters():
            param.requires_grad = False
        print("üîí GNN gel√©")
    
    def unfreeze_gnn(self):
        """D√©geler GNN"""
        for param in self.gnn.parameters():
            param.requires_grad = True
        print("üîì GNN d√©gel√©")
    
    def freeze_llm(self):
        """Geler LLM pour fine-tuning GNN seulement"""
        for param in self.llm.parameters():
            param.requires_grad = False
        print("üîí LLM gel√©")
    
    def unfreeze_llm(self):
        """D√©geler LLM"""
        for param in self.llm.parameters():
            param.requires_grad = True
        print("üîì LLM d√©gel√©")


def create_light_hybrid_model():
    """
    Cr√©er mod√®le hybride l√©ger complet
    
    - GNN: ~100K params
    - LLM: ~66M params (DistilBERT)
    - Projector + Fusion: ~600K params
    
    Total: ~170M params
    Entra√Ænables avec LoRA: ~1.1M params
    """
    
    from gnn_model_light import create_light_gnn
    from llm_wrapper_light import create_light_llm
    
    # Cr√©er composants
    gnn = create_light_gnn()
    llm = create_light_llm()
    
    # Cr√©er hybride
    hybrid_model = LightHybridModel(gnn, llm)
    
    # Stats
    total_params = sum(p.numel() for p in hybrid_model.parameters())
    trainable_params = sum(p.numel() for p in hybrid_model.parameters() if p.requires_grad)
    
    print("\n" + "="*60)
    print("‚úÖ MOD√àLE HYBRIDE L√âGER CR√â√â")
    print("="*60)
    print(f"  Total param√®tres: {total_params:,}")
    print(f"  Entra√Ænables: {trainable_params:,}")
    print(f"  Ratio: {trainable_params/total_params*100:.1f}%")
    print("="*60)
    
    return hybrid_model
